import os, sys
import numpy as np
import tensorflow as tf
from config_and_utils import GlobalVar, logging
from keras_preprocessing import image

PROJECT_DIR = GlobalVar.PROJECT_PATH
DATASET_DIR = GlobalVar.DATASET_PATH


def convert_to_tfrecords(data_npy, label_npy, target_path):
    with tf.python_io.TFRecordWriter(target_path) as writer:
        for i in range(len(data_npy)):
            features = tf.train.Features(
                feature={
                    # convert to string, which is byte data type
                    "data": tf.train.Feature(
                        bytes_list=tf.train.BytesList(value=[data_npy[i].tostring()])),
                    "label": tf.train.Feature(
                        bytes_list=tf.train.BytesList(value=[label_npy[i].tostring()]))
                }
            )
            example = tf.train.Example(features=features)
            serialized = example.SerializeToString()
            writer.write(serialized)


def parse_function(example_proto):
    features = {"data": tf.FixedLenFeature((), tf.string),
                "label": tf.FixedLenFeature((), tf.string)}
    parsed_features = tf.parse_single_example(example_proto, features)
    data = tf.decode_raw(parsed_features['data'], np.float16)
    label = tf.decode_raw(parsed_features['label'], np.float16)
    return data, label


def read_from_tfrecords(srcfile):
    """
    this function is created to help me test reading content from written tfrecords files.
    :param srcfile:
    :return:
    """
    sess = tf.Session()
    dataset = tf.data.TFRecordDataset(srcfile)  # load tfrecord file
    dataset = dataset.map(parse_function)  # parse data into tensor
    iterator = dataset.make_one_shot_iterator()
    next_data = iterator.get_next()

    for i in range(25):
        try:
            data, label = sess.run(next_data)
            if i == 24:
                data = data.reshape(144, 112, 1)
                label = label.reshape(144, 112, 1)
                img = image.array_to_img(data)
                lab = image.array_to_img(label)
                img.show()
                lab.show()
        except tf.errors.OutOfRangeError:
            pass
        finally:
            sess.close()


def convert_whole_dataset(dataset_dir_path='datasets/mri_pad_4'):
    """
    convert all train, validate, test .npy files in dataset generated by prepare_datasets.prepare_data()
    :param dataset_dir_path: path of dataset dir generated by prepare_datasets.prepare_dataset(), like "mri_pad_4"
    :return:
    """
    data_dir = dataset_dir_path + "/data"
    saving_dir = dataset_dir_path + "/tfrecords"

    if not os.path.exists(data_dir):
        logging.error("â—Error: " + data_dir + " doesn't exist")
        sys.exit(1)

    if not os.path.exists(saving_dir):
        logging.info("ğŸš©mkdir " + saving_dir)
        os.makedirs(saving_dir)

    if os.path.exists(data_dir + "/train_x.npy"):
        train_data = np.load(data_dir + "/train_x.npy")
        train_label = np.load(data_dir + "/train_y.npy")
        convert_to_tfrecords(train_data, train_label, saving_dir + "/train.tfrecords")
        logging.info("ğŸš©Done converting the train dataset.")

        test_data = np.load(data_dir + "/test_x.npy")
        test_label = np.load(data_dir + "/test_y.npy")
        convert_to_tfrecords(test_data, test_label, saving_dir + "/test.tfrecords")
        logging.info("ğŸš©Done converting the test dataset")

        validate_data = np.load(data_dir + "/validate_x.npy")
        validate_label = np.load(data_dir + "/validate_y.npy")
        convert_to_tfrecords(validate_data, validate_label, saving_dir + "/validate.tfrecords")
        logging.info("ğŸš©Done converting the validate dataset.")
    else:
        logging.error(
            "â—ï¸Error: default .npy files do not exists in " + data_dir +
            ", please run ./prepare_datasets.py to generate them")
        sys.exit(1)
